# KyroDB — Durable KV store with a Production Recursive Model Index (RMI)

**Status:** Alpha (focused scope: KV + RMI)

**One-liner:** KyroDB is a durable, append-only key-value engine with a production-grade learned index (RMI) for ultra-fast point lookups and predictable tail latency.

---

## At a glance

- RMI vs B-Tree point lookup microbench (median):

![RMI vs B-Tree (1M–300M keys)](bench/rmi_vs_btree.png)

Data: `bench/rmi_vs_btree_data.csv` (generated by `comp.py`). Reproduce steps below.

---

## TL;DR — Try it now

```bash
# 1) Run the engine (enable RMI features)
cargo run -p engine --features learned-index -- serve 127.0.0.1 3030

# 2) Health and offset
curl -s http://127.0.0.1:3030/health
curl -s http://127.0.0.1:3030/offset

# 3) Put/Get (HTTP)
curl -sX POST http://127.0.0.1:3030/put \
  -H 'content-type: application/json' \
  -d '{"key":123,"value":"hello"}'

curl -s "http://127.0.0.1:3030/lookup?key=123"

# 4) Optional: build the learned index (RMI)
curl -sX POST http://127.0.0.1:3030/rmi/build

# 5) Fast data-plane endpoints (for benchmarks)
curl -s "http://127.0.0.1:3030/lookup_fast/123"      # returns offset bytes
curl -s "http://127.0.0.1:3030/get_fast/123" | hexdump -C
```

Notes
- To require auth: start with `--auth-token <TOKEN>` and send `Authorization: Bearer <TOKEN>`.
- Release build: `cargo build -p engine --release` (binary at `target/release/kyrodb-engine`).
- Basic CLI (optional): `cd orchestrator && go build -o kyrodbctl .`

---

## Reproducible Benchmarks

See `bench/README.bench.md` for the full workflow. Highlights:

- Build release binaries with features used in papers/blogs:
  - `learned-index` enables the RMI read path and `/rmi/build`.
  - `bench-no-metrics` disables Prometheus counters in hot paths for purer numbers.
- Freeze index during the read phase: build once (`/rmi/build`), then avoid rebuilds.
- Capture artifacts to `bench/results/<commit>/` via `bench/scripts/capture.sh`.
- Adjust RMI via env vars: `KYRODB_RMI_TARGET_LEAF`, `KYRODB_RMI_EPS_MULT`.

Common commands:

```bash
# Microbench (in-process): compare RMI vs B-Tree
KYRO_BENCH_N=10000000 cargo bench -p bench --bench kv_index   # try 1e6, 1e7, 5e7, 3e8

# HTTP workload bench (read-mostly)
# 1) Start the engine (suggest: release, learned-index)
cargo run -p engine --features learned-index --release -- serve 127.0.0.1 3030

# 2) Load + read with the bench client, export CSV
# Example: 10M keys, 16-byte values, 64 concurrent readers, 30s read phase
COMMIT=$(git rev-parse --short HEAD)
cargo run -p bench --release -- \
  --base http://127.0.0.1:3030 \
  --load-n 10000000 \
  --val-bytes 16 \
  --concurrency 64 \
  --read-seconds 30 \
  --dist uniform \
  --out-csv bench/results/${COMMIT}/http_uniform_10m.csv

# 3) Generate plot + CSV for paper-style figures
python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt || pip install numpy matplotlib
python comp.py  # writes bench/rmi_vs_btree.png and bench/rmi_vs_btree_data.csv
```

Tips for fair numbers
- Pin CPU, keep system idle, disable turbo scaling if possible.
- Use `--features bench-no-metrics` when comparing tight hot-paths.
- Use `--release` and prefer the `lookup_fast`/`get_fast` routes in read tests.

---

## Why KyroDB (what we’re solving)

Many systems combine a log + key-value store + index and accept complexity, large memory overhead, or brittle tail latencies. KyroDB’s thesis:

> You can get a smaller, simpler, and faster single-binary KV engine by pairing an immutable WAL + snapshot durability model with a learned primary index (RMI) tuned for real workload distributions.

This repository is intentionally narrow: the primary goal is producing a production-grade KV engine (durability, compaction, recovery) + RMI implementation and reproducible benchmarks that demonstrate the advantages of learned indexes in real workloads.

---

## Primary use cases

- High-RPS point lookups where p99 predictability matters (feature stores, metadata services).
- Embedded single-binary KV for edge or microservices with on-disk persistence.
- Read-mostly workloads with occasional upserts, benefiting from compact on-disk layouts.
- Research/teaching reference for practical learned indexes with a real KV engine.

---

## Scope (what this repo contains now)

**IN SCOPE**

- Durable append-only Write-Ahead Log (WAL) with configurable fsync policy.
- Snapshotter with atomic swap for fast crash recovery.
- In-memory recent-write delta and a single-node read path.
- RMI scaffolding and an admin build endpoint; learned-index read path under active development.
- WAL size management via snapshots and truncation hooks.
- HTTP data plane for Put/Get and HTTP control plane (health, metrics, admin).
- `kyrodbctl` client for basic admin and dev workflows.

**OUT OF SCOPE (for now)**

- Vector search, ANN (HNSW), and vector-related APIs.
- Full SQL engine, complex query planner, joins, or multi-model features.
- Distributed clustering, replication, sharding.
- Production packaging beyond a single binary + Dockerfile (to be added once core is stable).

---

## Quickstart (developer)

Prereqs: Rust toolchain, (optional) Go for `kyrodbctl`.

Build and run engine (HTTP)

```bash
# from repo root
cargo run -p engine -- serve 127.0.0.1 3030
```

Build CLI (optional)

```bash
cd orchestrator
# build CLI binary in this folder
go build -o kyrodbctl .
```

Basic admin + KV demo (HTTP)

```bash
# health and offset
./kyrodbctl -e http://127.0.0.1:3030 health
./kyrodbctl -e http://127.0.0.1:3030 offset

# trigger a snapshot
./kyrodbctl -e http://127.0.0.1:3030 snapshot

# KV put via HTTP
curl -sX POST http://127.0.0.1:3030/put \
  -H 'content-type: application/json' \
  -d '{"key":123,"value":"hello"}'

# KV get via CLI (lookup by key)
./kyrodbctl -e http://127.0.0.1:3030 lookup 123
```

Notes:
- If you start the server with `--auth-token <TOKEN>`, pass `Authorization: Bearer <TOKEN>` headers in your HTTP requests; the CLI will add a flag for this soon.
- Release build: `cargo build -p engine --release` (binary at `target/release/engine`).

---

## Protocol decision (current)

- Data plane: HTTP/JSON for Put/Get (temporary while iterating).
- Control plane: HTTP/JSON endpoints for `/health`, `/metrics` (Prometheus), `/snapshot`, `/offset`, and `/rmi/build`.

gRPC for the data plane is planned (see Roadmap) but not implemented yet.

---

## Architecture (high level)

- **PersistentEventLog (WAL)** — append-only records with configurable fsync.
- **Snapshotter** — write new snapshot → atomic rename/swap → truncate WAL.
- **In-memory delta** — fast recent writes map checked before probing index.
- **RMI (learned index)** — builder and on-disk format under active development; read path will predict approximate position and do a bounded last-mile probe.
- **Compactor** — will build new snapshots with latest values and reclaim WAL space.

Simplified flow (current):

```
Client -> HTTP -> WAL.append -> mem-delta -> background snapshot
                 \-> Get -> mem-delta -> (future: RMI predict) -> last-mile probe -> disk
```

---

## What’s implemented (current status)

- WAL append + recovery ✅
- Snapshot + atomic swap ✅
- Cold start recovery (snapshot load + WAL replay) ✅
- In-memory delta and baseline read path ✅
- RMI scaffolding (admin build endpoint; on-disk loader WIP) ⚠️ in progress
- Compaction (keep-latest) ⚠️ planned
- Basic HTTP API + `kyrodbctl` for admin ✅

---

## Benchmarks (how we’ll present claims)

Bench scripts and results will live in `/bench`. The README will only present reproducible benchmarks with exact hardware and commit hashes. Primary comparisons: RMI vs a baseline B-Tree/rocks-like indexing on point-lookup workloads across uniform and Zipfian keys.

Example claim format (to be populated when results are published):

> On N keys (V-byte values) on machine X with SSD: RMI p99 read latency = X ms vs B-Tree p99 = Y ms (exact bench scripts and CSV in /bench/results).

Until the CSVs and run scripts are in the repo, treat any numbers as provisional.

---

## Roadmap (focus: KV + RMI)

**Short-term (now)**

- Harden WAL + snapshot atomicity and crash tests (fuzz/CI).
- Finalize RMI file layout (mmap-friendly) + versioning.
- Implement compaction + WAL truncation service.
- Publish reproducible benches for 10M and 50M keys.

**Mid-term**

- gRPC data-plane for Put/Get/Subscribe.
- Perf polish (prefetch, packed layouts), reduce probe tail latencies, autoswitch rebuild heuristics.
- Deliver Docker image + reproducible bench workflow.

**Long-term (after core is stable)**

- Consider range queries, optional supplemental B-Tree fallback, replication model, and selective vector features (as an experimental plugin).

---

## How to help / contribute

- Open issues with proposed changes and include tests (unit, property) when possible.
- When touching WAL/snapshot/RMI code paths, include regression/bench evidence.
- Share bench CSVs in `/bench/results/<your-name>` with machine specs for comparison.

---

## License

Apache-2.0

---

## Contact / Community

Open an issue for design discussions; label PRs that are experimental with `experimental`. For fast feedback ping @vatskishan03 on GitHub and Twitter(kishanvats03).
